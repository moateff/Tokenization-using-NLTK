# Input text
text = "Machine learning models are evolving rapidly. Tokenization is crucial in NLP tasks. "

# Tokenize by whitespace
tokens = text.split()

print(f"\nWhitespace Tokens:\n{tokens}\n")

# Whitespace Tokenization: A basic form of tokenization based solely on splitting by spaces.
# This is simple but not suitable for advanced NLP models.