# Input text
text = "Character Tokenization."

# Character Tokenization
char_tokens = list(text)

print(f"\nCharacter Tokens:\n{char_tokens}\n")


# Character Tokenization: Splitting the text into characters can be useful for language generation
# models or when fine-grained input control is needed.
